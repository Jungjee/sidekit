Run an `i`-vector system
========================
   
This script runs an experiment on the male NIST Speaker Recognition
Evaluation 2010 extended core task.
For more details about the protocol, refer to the NIST_ website.

The complete Python script can be downloaded :download:`here <sre10_i-vector.zip>`

In order to get this scirpt running on your machine, you will need to modify a limited number of 
options to indicate where your features are located and how many threads you want to run in parallel.


Getting ready
-------------

Load your favorite modules before going further.:: 
   
   import sys
   import numpy as np
   import scipy
   
   import os
   import copy
   
   import sidekit
   import multiprocessing
   import matplotlib.pyplot as mpl
   import logging
   logging.basicConfig(filename='log/sre10_i-vector.log',level=logging.INFO)
   logging.basicConfig(level=logging.DEBUG)

Define now your local directories.

- **expe_root_dir** is the root directory where your experiments data will be stored (temporary files, score files statistics... and so on...)
- **task_dir** is the directory where are stored the **key** files, the **IdMap**, the list of files to use for the UBM training and the **Ndx** generated by the :ref:`sre10\_init.py <sre10init>` script.
- **i4U_dir** is the directry that contains the metadata files from the I4U consortium.::

   expe_root_dir = '/lium/parolee/larcher/src/python/sidekit_tutorial/nist-sre/'
   task_dir = '/lium/parolee/larcher/src/python/sidekit_tutorial/nist-sre/task'
   i4U_dir = '/lium/parolee/larcher/src/python/sidekit_tutorial/nist-sre/Sph_MetaData'
 
Create the sub-directories if they don't exist.::
   
   if not os.path.exists(expe_root_dir):
       os.makedirs(expe_root_dir)
   os.chdir(expe_root_dir)
   
   log_dir = os.path.join(expe_root_dir,'log')
   gmm_dir = os.path.join(expe_root_dir,'gmm')
   data_dir = os.path.join(expe_root_dir,'data')
   scores_dir = os.path.join(expe_root_dir,'scores')
   
   if not os.path.exists(log_dir):
       os.makedirs(log_dir)
   if not os.path.exists(gmm_dir):
       os.makedirs(gmm_dir)
   if not os.path.exists(data_dir):
       os.makedirs(data_dir)
   if not os.path.exists(scores_dir):
       os.makedirs(scores_dir)

Here is the important section which defines what the script will be doing.
For conveniences you can select to :

- train the `i`-vector system (including UBM, TV and `i`-vector extraction.
- run the test phase for which you can chose to perform:
   - cosine scoring
   - mahalanobis scoring
   - 2-covariance scoring
   - PLDA scoring
- plot the resulting DET curves for the different systems

You should also set the number of distribution for the UBM and the rank of the TV matrix.
Eventually, indicate the root directory where your feature files are stored (see :ref:`sre10\_init.py <sre10init>` 
for more information. The number of parallel process is fixed automatically to the number of threads available minus one.::

   #################################################################
   # Set your own parameters
   #################################################################
   train = True  # if True, train th UBM, TV matrux and extract the i-vectors
   test = True  # if True, perform the experiments xonsidering all i-vectors already exist
   plot = True  # if True, plot the DET curve with all systems in condition 5
   distribNb = 1024  # number of Gaussian distributions for the UBM
   rank_TV = 400  # Rank of the Total Variability matrix
   audioDir = '/lium/parolee/larcher/data/nist/'  # Root directory where features are stored
   scoring = ['cosine', 'mahalanobis', '2cov', 'plda'] # list of scoring to run on the task, could be 'cosine', 'mahalanobis', '2cov' or 'plda'
   
   # Automatically set the number of parallel process to run.
   # The number of threads to run is set equal to the number of cores available 
   # on the machine minus one or to 1 if the machine has a single core.
   nbThread = max(multiprocessing.cpu_count()-1, 1)

The next section loads all lists, Ndx, IdMap and Keys required.::

   #################################################################
   # Load IdMap, Ndx, Key from PICKLE files and ubm_list
   #################################################################
   print('Load task definition')
   enroll_idmap = sidekit.IdMap('task/sre10_coreX-coreX_m_trn.h5', 'hdf5')
   nap_idmap = sidekit.IdMap('task/sre04050608_m_training.h5', 'hdf5')
   back_idmap = sidekit.IdMap('task/sre10_coreX-coreX_m_back.h5', 'hdf5')
   test_ndx = sidekit.Ndx('task/sre10_coreX-coreX_m_ndx.h5', 'hdf5')
   test_idmap = sidekit.IdMap('task/sre10_coreX-coreX_m_test.h5', 'hdf5')
   keys = []
   for cond in range(9):
       keys.append(sidekit.Key('task/sre10_coreX-coreX_det{}_key.h5'.format(cond + 1)))
   
   with open('task/ubm_list.txt', 'r') as inputFile:
       ubmList = inputFile.read().split('\n')

Train your `i`-vector etractor
------------------------------

Train the `i`-vector system. First, define a FeatureServer to load alreday extracted features stored in SPRO4 format.::    

   if train:
       #%%
       #################################################################
       # Process the audio to generate MFCC
       #################################################################
       print('Create the feature server to extract MFCC features')
       fs = sidekit.FeaturesServer(input_dir=audioDir,
                    input_file_extension='.mfcc',
                    label_dir='./',
                    label_file_extension='.lbl',
                    from_file='spro4',
                    config='sid_8k',
                    keep_all_features=False)
   
Train the UBM. The features are process in parallel.::
The Mixture object is then save in **pickle** format.::

       print('Train the UBM by EM')
       ubm = sidekit.Mixture()
       llk = ubm.EM_split(fs, ubmList, distribNb, numThread=nbThread)
       ubm.save_pickle('gmm/ubm_tandem.p')
 
The UBM is now used to compute the zero and first order sufficient statistics
that are then saved to disk in HDF5 format.::

       print('Compute the sufficient statistics')
       # Create a StatServer for the enrollment data and compute the statistics
        enroll_stat = sidekit.StatServer(enroll_idmap, ubm)
       enroll_stat.accumulate_stat(ubm=ubm, feature_server=fs, seg_indices=range(enroll_stat.segset.shape[0]), numThread=nbThread)
       enroll_stat.save('data/stat_sre10_coreX-coreX_m_enroll.h5')
        
       nap_stat = sidekit.StatServer(nap_idmap, ubm)
       nap_stat.accumulate_stat(ubm=ubm, feature_server=fs, seg_indices=range(nap_stat.segset.shape[0]), numThread=nbThread)
       nap_stat.save('data/stat_sre04050608_m_training.h5')
       
       test_stat = sidekit.StatServer(test_idmap, ubm)
       test_stat.accumulate_stat(ubm=ubm, feature_server=fs, seg_indices=range(test_stat.segset.shape[0]), numThread=nbThread)
       test_stat.save('data/stat_sre10_coreX-coreX_m_test.h5')
       
Next step is to train the TotalVariability matrix.
10 iterations of EM algorithm are performed with minimum divergence step.
Only the Covariance is re-estimated.::
   
       print('Estimate Total Variability Matrix')
       mean, TV, _, __, Sigma = nap_stat.factor_analysis(rank_TV,
                           itNb=(10,0,0), minDiv=True, ubm=ubm, 
                           batch_size=1000, numThread=nbThread)
          
       sidekit.sidekit_io.write_pickle(TV, 'data/TV_sre04050608_m.p')
       sidekit.sidekit_io.write_pickle(mean, 'data/TV_mean_sre04050608_m.p')
       sidekit.sidekit_io.write_pickle(Sigma, 'data/TV_Sigma_sre04050608_m.p')

Parameters of the **factor_analysis** method are:

- the rank of the total variability matrix
- a tuple of 3 integers, used for JFA estimation. here only the first component will be used
- **minDiv** a boolean that controlled the use of te minimum divergence step
- **ubm** a Mixture object which mean and co-variance parameters ill be used
- **batch_size** an integer that fix the maximum number of sessions to process at the same time
  (the lower, the less memory used)
- **numThread** the number of process to run in parallel

The **mean** vector, **TV** matrix and **Sigma** are saved to disk.

.. note::
   **mean** and **Sigma** are directly taken from the UBM model

The resulting `i`-vector extractor is used to extract `i`-vectors on the different sets of data.
The i-vectors are then saved to disk as StatServer in HDF5 format.:: 

       print('Extraction of i-vectors') 
       enroll_iv = enroll_stat.estimate_hidden(mean, Sigma, V=TV, U=None, D=None, numThread=nbThread)[0]
       enroll_iv.save('data/iv_sre10_coreX-coreX_m_enroll.h5')
       
       test_iv = test_stat.estimate_hidden(mean, Sigma, V=TV, U=None, D=None, numThread=nbThread)[0]
       test_iv.save('data/iv_sre10_coreX-coreX_m_test.h5')
       
       nap_iv = nap_stat.estimate_hidden(mean, Sigma, V=TV, U=None, D=None, numThread=nbThread)[0]
       nap_iv.save('data/iv_sre04050608_m_training.h5')


Run the tests
-------------

The test step is performed as follow. First the `i`-vectors are loaded in StatServers.

- one StatServer for enrolement data
- one StatServer for training data
- one StatServer for test segments

::

   if test:
   
       enroll_iv = sidekit.StatServer('data/iv_sre10_coreX-coreX_m_enroll.h5')
       nap_iv = sidekit.StatServer('data/iv_sre04050608_m_training.h5')
       test_iv = sidekit.StatServer('data/iv_sre10_coreX-coreX_m_test.h5')


Using Cosine similarity
^^^^^^^^^^^^^^^^^^^^^^^

If the scoring list includes 'cosine', different flavors of the Cosine scoring are performed.::

       if 'cosine' in scoring:

A simple cosine scoring without any normalization of the i-vectors.::

           print('Run Cosine scoring evaluation without WCCN')
           scores_cos = sidekit.iv_scoring.cosine_scoring(enroll_iv, test_iv, test_ndx, wccn = None)
           scores_cos.save('scores/scores_cosine_sre10_coreX-coreX_m.h5')

A version where `i`-vectors are normalized using Within Class Covariance normalization (WCCN).::

           print('Run Cosine scoring evaluation with WCCN')
           wccn = nap_iv.get_wccn_choleski_stat1()
           scores_cos_wccn = sidekit.iv_scoring.cosine_scoring(enroll_iv, test_iv, test_ndx, wccn=wccn)
           scores_cos_wccn.save('scores/scores_cosine_wccn_sre10_coreX-coreX_m.h5')

The same with a Linear Discriminant Analysis performed first to reduce the dimension of `i`-vectors to 150 dimensions.::

           print('Run Cosine scoring evaluation with LDA')
           LDA = nap_iv.get_lda_matrix_stat1(150)
           
           nap_iv_lda = copy.deepcopy(nap_iv)
           enroll_iv_lda = copy.deepcopy(enroll_iv)
           test_iv_lda = copy.deepcopy(test_iv)
           
           nap_iv_lda.rotate_stat1(LDA)
           enroll_iv_lda.rotate_stat1(LDA)
           test_iv_lda.rotate_stat1(LDA)
           
           scores_cos_lda = sidekit.iv_scoring.cosine_scoring(enroll_iv_lda, test_iv_lda, test_ndx, wccn=None)
           scores_cos_lda.save('scores/scores_cosine_lda_sre10_coreX-coreX_m.h5')
           
           print('Run Cosine scoring evaluation with LDA + WCCN')
           wccn = nap_iv_lda.get_wccn_choleski_stat1()
           scores_cos_lda_wcnn = sidekit.iv_scoring.cosine_scoring(enroll_iv_lda, test_iv_lda, test_ndx, wccn=wccn)
           scores_cos_lda_wcnn.save('scores/scores_cosine_lda_wccn_sre10_coreX-coreX_m.h5')

Using Mahalanobis distance
^^^^^^^^^^^^^^^^^^^^^^^^^^

If the scoring list includes 'mahalanobis', `i`-vectors are normalized using one iteration of the Eigen Factor Radial algorithm (equivalent to the so called length-normalization). Then scores are computing using a Mahalanobis distance.::
   
       if 'mahalanobis' in scoring:
       
           print('Run Mahalanobis scoring evaluation with 1 iteration EFR')
           meanEFR, CovEFR = nap_iv.estimate_spectral_norm_stat1(3)
        
           nap_iv_efr1 = copy.deepcopy(nap_iv)
           enroll_iv_efr1 = copy.deepcopy(enroll_iv)
           test_iv_efr1 = copy.deepcopy(test_iv)
       
           nap_iv_efr1.spectral_norm_stat1(meanEFR[:1], CovEFR[:1])
           enroll_iv_efr1.spectral_norm_stat1(meanEFR[:1], CovEFR[:1])
           test_iv_efr1.spectral_norm_stat1(meanEFR[:1], CovEFR[:1])
           M1 = nap_iv_efr1.get_mahalanobis_matrix_stat1()
           scores_mah_efr1 = sidekit.iv_scoring.mahalanobis_scoring(enroll_iv_efr1, test_iv_efr1, test_ndx, M1)
           scores_mah_efr1.save('scores/scores_mahalanobis_efr1_sre10_coreX-coreX_m.h5') 

Using a Two-covariance scoring
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the scoring list includes '2cov', two 2-covariance models are trained with and without `i`-vector normalization.
The normalization applied consists of one iteration of Spherical Noramlization.::
   
       if '2cov' in scoring:
       
           print('Run 2Cov scoring evaluation without normalization')
           W = nap_iv.get_within_covariance_stat1()
           B = nap_iv.get_between_covariance_stat1()
           scores_2cov = sidekit.iv_scoring.two_covariance_scoring(enroll_iv, test_iv, test_ndx, W, B)
           scores_2cov.save('scores/scores_2cov_sre10_coreX-coreX_m.h5')
       
           print('Run 2Cov scoring evaluation with 1 iteration of Spherical Norm')
           meanSN, CovSN = nap_iv.estimate_spectral_norm_stat1(1, 'sphNorm')
       
           nap_iv_sn1 = copy.deepcopy(nap_iv)
           enroll_iv_sn1 = copy.deepcopy(enroll_iv)
           test_iv_sn1 = copy.deepcopy(test_iv)
       
           nap_iv_sn1.spectral_norm_stat1(meanSN[:1], CovSN[:1])
           enroll_iv_sn1.spectral_norm_stat1(meanSN[:1], CovSN[:1])
           test_iv_sn1.spectral_norm_stat1(meanSN[:1], CovSN[:1])
       
           W1 = nap_iv_sn1.get_within_covariance_stat1()
           B1 = nap_iv_sn1.get_between_covariance_stat1()
           scores_2cov_sn1 = sidekit.iv_scoring.two_covariance_scoring(enroll_iv_sn1, test_iv_sn1, test_ndx, W1, B1)
           scores_2cov_sn1.save('scores/scores_2cov_sn1_sre10_coreX-coreX_m.h5')
   
Using a Probabilistic Linear Dscriminant Analysis (PLDA)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the scoring list includes 'plda', two experiments are run using a PLDA mode with and without applying one iteration of the EFR algorithm to normalize the `i`-vectors.::

       if 'plda' in scoring:
        
           print('Run PLDA scoring evaluation without normalization')    

Estimate the mean and covariance used for the EFR normalization.::

           meanSN, CovSN = nap_iv.estimate_spectral_norm_stat1(1, 'efr')

Create a new version of the `i`-vectors that will be then noralized using EFR.::

           nap_iv_sn1 = copy.deepcopy(nap_iv)
           enroll_iv_sn1 = copy.deepcopy(enroll_iv)
           test_iv_sn1 = copy.deepcopy(test_iv)
           
           nap_iv_sn1.spectral_norm_stat1(meanSN[:1], CovSN[:1])
           enroll_iv_sn1.spectral_norm_stat1(meanSN[:1], CovSN[:1])
           test_iv_sn1.spectral_norm_stat1(meanSN[:1], CovSN[:1])

Train a PLDA model by 10 iterations of EM with minimum divergence step.
Parameters of the **factor_analysis** method are:

- the rank of the EigenVoice matrix of the PLDA
- the rank of the EigenChannel matrix (here set to 0 as we use a simplified version of the PLDA)
- **re_estimate_residual** a boolean set to True to indicae that we re-estimate the residual **Sigma** covariance matrix at each iteration (which is not the case when training the `i`-vector extractor where the covariance of the UBM is kept fixed for the entire process.
- a tuple of 3 integers where the first one defines the number of iterations to perform to train the EigenVoice matrix and the second one gives the number of iterations to estimate the EigenChannel matrix. The third dimension is not used here.
- **batch_size** an integer that fix the maximum number of sessions to process at the same time  (the lower, the less memory used)  
- **numThread** the number of process to run in parallel 

::

           nap = copy.deepcopy(nap_iv)
           nap_sn = copy.deepcopy(nap_iv_sn1)
           
           print('Run PLDA rank = 400, 10 iterations without normalization'.format(rk, it))
           mean, F, G, _, Sigma = nap.factor_analysis(rk, rank_G=0,
                           re_estimate_residual=True,
                           itNb=(it,0,0), minDiv=True, ubm=None,
                           batch_size=1000, numThread=nbThread)
           print('scoring')
 
Compute all the scores defined in the **test_ndx** Ndx object. **enroll_iv** contains all `i`-vectors fr enrolment,
**test_iv** contains the `i`-vectors for each test segment.::

           scores_plda = sidekit.iv_scoring.PLDA_scoring(enroll_iv, test_iv, test_ndx,
                                                 mean, F, G, Sigma)

The scores are saved to disk in HDF5 format.::

           scores_plda.save('scores/scores_plda_rank400_it10_sre10_coreX-coreX_m.h5')

Perform another experiment using PLDA trained after one itertion of the EFR algorithm.::

           print('Run PLDA rank = 400, 10 iterations with 1 iteration of Eigen Factor Radial')
           mean1, F1, G1, _, Sigma1 = nap_sn.factor_analysis(rk, rank_G=0, rank_H=None,
                           re_estimate_residual=True,
                           itNb=(it,0,0), minDiv=True, ubm=None,
                           batch_size=1000, numThread=nbThread)
           scores_plda_efr1 = sidekit.iv_scoring.PLDA_scoring(enroll_iv_sn1, test_iv_sn1, test_ndx, mean1, F1, G1, Sigma1)
           scores_plda_efr1.save('scores/scores_plda_rank_400_it10_efr1_sre10_coreX-coreX_m.h5')
   
Plot the DET curves
-------------------

In case you want to display the results of the experiments. First define the target prior, the parameters of the graphic window and the title of the plot.::

   if plot:
       print('Plot the DET curve')
       # Set the prior following NIST-SRE 2010 settings
       prior = sidekit.effective_prior(0.001, 1, 1)
       # Initialize the DET plot to 2010 settings
       dp = sidekit.DetPlot(windowStyle='sre10', plotTitle='I-Vectors SRE 2010-ext male, cond 5')

For each of the performed experiments, load the target and non-target scores for the condition 5 according to the key file.::

       dp.set_system_from_scores(scores_cos, keysX[4], sys_name='Cosine')
       dp.set_system_from_scores(scores_cos_wccn, keysX[4], sys_name='Cosine WCCN')
       dp.set_system_from_scores(scores_cos_lda, keysX[4], sys_name='Cosine LDA')
       dp.set_system_from_scores(scores_cos_wccn_lda, keysX[4], sys_name='Cosine WCCN LDA')
       
       dp.set_system_from_scores(scores_mah_efr1, keysX[4], sys_name='Mahalanobis EFR')
        
       dp.set_system_from_scores(scores_2cov, keysX[4], sys_name='2 Covariance')
       dp.set_system_from_scores(scores_2cov_sn1, keysX[4], sys_name='2 Covariance Spherical Norm')
       
       dp.set_system_from_scores(scores_plda, keysX[4], sys_name='PLDA')
       dp.set_system_from_scores(scores_plda_efr, keysX[4], sys_name='PLDA EFR')

Create the window and plot::

       dp.create_figure()
       dp.plot_rocch_det(0)
       dp.plot_rocch_det(1)
       dp.plot_rocch_det(2)
       dp.plot_rocch_det(3)
       dp.plot_rocch_det(4)
       dp.plot_rocch_det(5)
       dp.plot_rocch_det(6)
       dp.plot_rocch_det(7)
       dp.plot_rocch_det(8)
       dp.plot_DR30_both(idx=0)
       dp.plot_mindcf_point(prior, idx=0)
   

Depending of the data available, the following plot could be obtained at the end of this tutorial:
(For this example, data used include NIST-SRE 04, 05, 06, 08, the SwitchBoard Part 2 phase 2 and 3 and Cellular part 2)
Those results are far from optimal as don't generalize on other conditions of NIST-SRE 2010. This system has been 
trained without any specific data selection and its purpose is only to give an idea of what you can obtain.

.. figure:: I-Vector_sre10_cond5_male_coreX.png

.. _NIST: http://www.itl.nist.gov/iad/mig/tests/sre/2010/
